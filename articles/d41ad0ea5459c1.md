---
title: "[エンジニア向け]Google ColabとKaggleを攻略"
emoji: "🙌"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: [Kaggle,GllgeColab,Python]
published: true
---

# 【エンジニア向け】Google ColabでKaggleを攻略
ある程度、ソフトウエア開発の関連知識がある方に向けて、GoogleColabを利用してAIコンペのKaggleに挑戦する方法を解説します。

---

## 1 GoogleColabについて

Googleが2017年頃から提供する仮想環境です。GCPとは独立していますが、課金で連携も可能です。
![](/images/Gemini_Generated_Image_qmpxu1qmpxu1qmpx.png)
GoogleのIDが在れば、だれでも無料で利用が可能です（高機能な有料版もあります）。

主な用途は、
- 小〜中規模の機械学習実験
- GPU/TPU を用いた短期の実験
- 教育・チュートリアルの配布や共有
- Kaggle の素早い検証・ベースライン実行

などです。

### サービスの基本
GoogleColabは**ジュピターノートブック**と呼ばれるファイルをWebで操作します。このファイルは基本的にGoogleDriveに保存され、ノートには以下の２つを含みます。

- 処理の実行できるコードブロック
- 文章の入るテキストブロック

これらを適宜、組み合わせて使用します。

### 仮想環境について

Colabが提供するランタイムは、特定のLinuxディストリビューション（通常は Ubuntuの特定のバージョン）に基づくコンテナです。
この環境はGoogleによってホスト・管理されており、ユーザーがOSレベルの根本的な変更（ディストリビューションの切り替えなど）を行うためのオプションは提供されていません。

ただし、提供されている環境内では、`apt` などのパッケージマネージャーを使用して追加のソフトウェアやライブラリをインストールしたり、shellコマンドを実行したりすることは可能です。

### 継続時間などの制限がある
課金状態で変わりますが、一定の制限があります。リソースの保証なども欲しい場合は課金を検討してください。

### KaggleとGoogleColabでのVMの違い
コンペを提供しているKaggleのWebでも、GoogleColabと同様にVMがありますが、中身は違います。AIによるサジェストの有無などを較べるとGoogleColabの方が便利です。一方、kaggleではコンペのデータ読み込みや提出が簡単というメリットがあります。

---

## 2 Pythonのハック
Python利用歴のある方は読み飛ばしていただいて結構です。他言語との主な違いを述べます。

### 基本的な位置づけ

- 基本動作はインタープリター・動的型付け
- 実行は行単位で、基本的に全体をコンパイルしません
- 動的型付けの言語で、変数の型がプログラムの実行時に決定されます
  
よくあるサーバサイドで用いるスクリプト型とあまり変わりません。コンパイルする言語とは違いがあります。

```Python
s = "sss" # s に文字列型（str）の値が代入される
s = 1     # s に整数型（int）の値が再代入される
```
こうした処理もエラーになりません。

### 変更可能（ミュータブル）、変更不可能（イミュータブル）

Pythonのオブジェクトは、作成後にその中身を変更できるかどうかで、イミュータブル（Immutable: 変更不可能）とミュータブル（Mutable: 変更可能）に分類されます。

- 数値や文字列（プリミティブ）: イミュータブル（変更不可能）
- リストや辞書（コレクション）: ミュータブル（変更可能）

こうした振る舞いはC#等の言語とあまり変わりません。

### `()`,`[]`,`{}`の使い分け
配列に類する特定の型宣言になります、それぞれ以下になります。

| 記号 | 名称 | 主な用途 | 特徴 |
|---|---|---|---|
| () | 丸括弧 | タプル、関数の呼び出し/定義、優先順位 | イミュータブル (タプル) |
| [] | 角括弧 | リスト、インデックス指定/スライス | ミュータブル (リスト) |
| {} | 波括弧 | 辞書 (Dict)、セット (Set) | ミュータブル (Dict, Set) |


### importとインストールの方法
利用したいパッケージによって`import`と`install`を使います。

#### Pythonのシステムに備わっているものは `import`
よくある例を以下に示します。
```python
import pandas as pd
import  numpy as np
import matplotlib.pyplot as plt
```

ここで、
```python
import □□□□□ as ◯◯
```
と記した場合、
> 「`□□□□□`をimportして〇〇として略して参照する」

という処理を実行します。

#### 外部から取り入れる`install`
外部のライブラリを用いる場合、`install`をLinuxコマンドとして実行します。

##### コマンドを実行するふたつの書き方

###### セル全体がコマンドの場合`%%bash`
先頭行に`%%bash`があった場合、記述された内容を仮想環境のLinuxコマンドとして実行します。

```bash
%%bash
pip install japanize-matplotlib
kaggle competitions download -c m5-forecasting-accuracy --force
```
##### 一行コマンドの場合`！`を先頭につける
`！`がある行だけLinuxコマンドとして実行され、他はPythonで実行されます。

```python
!pip install japanize-matplotlib
import japanize_matplotlib
japanize_matplotlib.japanize()
```
尚、ここで使っている`pip`はPythonでのパッケージを管理するシステムです。

### インデントとブロック
インデントによってブロックが形成されます。慣例的にスペース4つを使います。

以下は`for`の例です。
```python
for train_idx, val_idx in kf.split(X_train, y_train):
    x_tr, y_tr = X_train.iloc[train_idx], y_train.iloc[train_idx]
    x_va, y_va = X_train.iloc[val_idx], y_train.iloc[val_idx]
    model = CatBoostClassifier(**params)
```
以下はclass宣言の例です。
```python
class Paths:
    P = "/content/kaggle/"
    train = P + "train.csv"
    test = P + "test.csv"
    sample = P + "sample_submission.csv"
```

### .と（）
`.`は一般的なオブジェクト参照と同じくメンバ参照になります。
```py
□□□□□.◯◯
```
また、`（）`はメソッドです。
```python
aa = bbb.ccc.ddd(x,y,z)
```

## 3 実践Tips
GoogleColabを利用する際、使えると便利な機能や、仮想環境を意識した利用方法のヒントを以下に示します。

### 見出しで区切ると畳める
テキストブロックで見出しを作っておくと、見出し単位で折りたたみができます。
![](/images/SS_noteTitle.png)
こうしておくことで、もういじらなくて良い部分は隠して、長々とスクロールすることを防げます。
よくあるIDEのような、コードブロック内でのネストしたコードのブロックを畳む機能はありません。それが良い場合は他のIDEを利用しましょう。

### 変更履歴の利用
簡易的なバージョン管理機能があります。「版を固定して保存」すると履歴にラベルが付けられます。
![](/images/ss_SaveTag.png)
これを用いると、その保存したバージョンに名前が残せます。別名で保存することで版を固定することも可能ですが、どこを編集したのか、差分を確認できるのが便利なところです。
![](/images/SS_CodeCompare.png)
これで変更箇所の確認もでき、場合によってはそのバージョンに簡単に遡って、無駄になった変更を破棄することも可能です。
GoogleColabはGithubのリポジトリに接続することも可能なので、慣れてる方はそちらでバージョン管理することもできます。

更に、Kaggleへ提出したデータがログで判るようにすると、どのノートから出力したファイルが幾つのスコアだったのか分かりやすくなります。以下はファイル名にタイムスタンプを残す方法です。Submitにも時刻が残りますが表記がざっくり何日前とかで、実際に照合しにくいからです。

```python
from datetime import datetime

# 現在の日時を取得してフォーマット
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# ファイル名に日付と時刻を追加
filename = f"submission_{timestamp}.csv"

# CSV を保存
submission_merged.to_csv(filename, index=False)

# Kaggle へ提出
!kaggle competitions submit -c m5-forecasting-accuracy -f {filename} -m "upload"
print(f"Submission saved as {filename}")
```
セルの出力は以下になります。

```セルの出力
100% 20.5M/20.5M [00:00<00:00, 54.9MB/s]
Successfully submitted to M5 Forecasting - AccuracySubmission saved as submission_20251019_080545.csv
```

KaggleのSubmitにメモが残せるので、それも有効活用しましょう。
![](/images/SS_KaggleScoreMemo.png)

頻繁にコードの修正と取り消しが発生します、面倒になって初めからやり直すのも可能ですが、新規チャットでやると途中でLLMに尋ねてると違う応答になり、再現できないこともあります。

### LLMに作らせる
LLMによるコーディングが一般化しているので、「今更、コードを地道に打つのは面倒」と思う人が大多数でしょう。そうした場合の利用方法です。

#### コーディングのお助け
GoogleColabでは設定していれば、簡単にコーディングをサジェストしてくれます。
Colab内のGeminiに日本語入力したい場合、特に面倒なのが、**日本語変換でEnterキーを押すと送信される**という現象です。これは他で入力してコピペするなどが要りますが、とても不便です。全体を見直したい場合、他のタブでLLMを開いてノートを読み込ませ、指示するなどでやる方法も有効です。

#### エージェントにやらせる（やれる人向け）
ノートはブラウザ上で編集するので、もし利用できるエージェントがあるなら、代行で編集させることが可能です。ノートをDLしVScodeでCopilotのエージェントに編集させることもできます。SSHで接続なども在ると思いますが、適宜、方法を探してください。

### メモリ不足・遅い処理への対処
学習するデータフレームが大きくなると、自ずと処理に時間がかかります。大きくなりすぎてメモリ不足で実行できなくなることも頻繁になることもあります。
![](/images/SS_Crush.png)
そうした場合の対処方法です。

#### 仮想マシンの変更
課金のProを利用している場合、環境の変更が可能です。
![](/images/SS_RuntimeSetteing.png)
まず、ハイメモリ設定にしましょう。また、GPUが利用できるライブラリの場合は、以下を実行しましょう。

1. ハードウェアアクセラレータを指定、GPUを有効に
2. スクリプトでGPUを有効化

GPU内のVRAMにデータを移せる部分があれば、メモリ不足の解消と離散処理が高速化する可能性があります。

#### 無用なデータを消す

処理ごとに、そのときに必要なデータ以外は破棄すると、メモリに空きが増やせます。例えば、入力するデータの構築で用いたデータに個人名簿、カレンダー、個人の購買行動がそれぞれ独立したデータフレームに存在した場合、学習に使うのが合体したものであれば、他は明示的に消すことでメモリの空きが増やせます。

```python
# 例: マージ後に元の DataFrame を削除してメモリを解放する
import gc

# ここでは df_left, df_right をキー 'id' でマージして df に集約する例を示します。
# 実際のキーや how はデータ構造に合わせて変更してください。
df = df_left.merge(df_right, on='id', how='left')

# マージ後に元の参照を削除してガベージコレクションを促す
del df_left, df_right
gc.collect()

# 必要に応じて、明示的に不要列を削除してメモリ削減
# df.drop(columns=['tmp_col1','tmp_col2'], inplace=True)
```

ガベージコレクションがPythonには備わってますが、全体のスコープがrootになり広くなりがちなノートでは、インタープリターとして実行時に参照が外れることが起こりにくいので、意図的に消去する必要があります。

#### パラメータの変更
簡単に出来るのは、パラメータでのモデルの複雑さを減らすなどです。これは性能が下がる可能性もあり躊躇しますが、無駄に深い場合もあるので検討しましょう。ニューラルネットでは畳み込み方法にも関連します。

#### 学習データ量の検討
気持ち的には全部のデータを学習に用いたいところですが、そのためにメモリがパンクするようだと困ります。データの削減を試みます。

##### ラベルを減らす
これはモデルの改善と同じで考える部分で、従属関係のあるラベルやノイズになっているものを削減する方法です。やりすぎても性能が下がることはあるので、注意をはらいながら削減しましょう。

##### 行を減らす
元のデータの件数を減らす方法です。これもやり方が難しいですが、時系列なら昔のデータは使わない、など、影響度を考えて試しましょう。

##### データを分割する
特定のラベルごとにまとめたり、行を分けてまとめて小さなデータフレームに分割します。それぞれで学習するならば必要なメモリ量は減らせます。分割してできた複数モデルの平均などを用いる方法です。

##### データ・タイプの変更
数値の場合、必要なバイト数が異なります。できるだけ削減します。

以下は NumPy 型と bool の代表的な数値型一覧です。

| 型 | 小数・整数 | 範囲（目安） | バイト数（格納領域の目安） |
|---|---:|---|---:|
| numpy.int8 | 整数 | -128 〜 127 | 1 |
| numpy.int16 | 整数 | -32,768 〜 32,767 | 2 |
| numpy.int32 | 整数 | -2,147,483,648 〜 2,147,483,647 | 4 |
| numpy.int64 | 整数 | 約 -9.22e18 〜 9.22e18（約 ±2^63） | 8 |
| numpy.float32 | 小数（単精度） | 約 ±3.4×10^38 | 4 |
| numpy.float64 | 小数（倍精度） | 約 ±1.8×10^308 | 8 |
| bool / numpy.bool_ | 真偽（論理値） | True / False | 1（概念上） |

注記:
- 上の「バイト数」は各値を格納するデータ幅の目安です。NumPy 配列は同じ型を連続で並べるため、Python の組み込み `list` や CPython オブジェクトに比べメモリ効率が良くなります。
- 金額など誤差を許さない小数計算には `decimal.Decimal` を検討してください（ただし NumPy の浮動小数点と比べて計算は遅く、別途扱いが必要です）。

変更する場合は以下で考えて試しましょう。

- 整数で良いものは整数にする
- 値の範囲が小さいものはバイト数が少ないものに寄せる

さらに、値によっては標準化や対数にしたら少なく出来る場合もあるので、検討します。
究極的には数値の組み合わせによって1ビットに意味をもたせる方法（ワンホットベクトルをビット単位にするなど）もありますが、アクセスや処理の複雑さが増すため効率を落とすことがあり、やり過ぎには注意が必要です。

**GPUを利用する場合はfloat32で組むとバウンダリで無駄な領域が省け、転送に無駄が減ります。**


#### 段階的な学習
部分ごとに分けて学習するインクリメンタル学習やアンサンブルを用い、読み込むデータやモデルのサイズを減らします。以下はインクリメンタルの例。
`batch_size=1000`の部分が分割単位です。

```python
# 例: Scikit-learnでの部分的な学習
from sklearn.linear_model import SGDClassifier
import numpy as np

model = SGDClassifier(loss='log_loss')
classes = np.unique(y_all) # すべてのクラスを事前に指定する必要がある

# データ生成（実際には大きなデータセットからバッチを読み込む）
data_batches = generate_data_in_batches(X_all, y_all, batch_size=1000)

for X_batch, y_batch in data_batches:
    model.partial_fit(X_batch, y_batch, classes=classes)

# 最終的に 'model' がすべてのバッチから学習した単一のモデルとなります。

```
インクリメンタルでやるときとまとめてフルバッチ学習する場合では、差ができることがあるので注意が必要です。離散的に処理が可能なニューラルネットでは差分が出にくいですが、勾配法の場合は個別で動作するモデルに近い状況になります。

### 稼働時間が長い
データ量を削減すれば概ね完了までの時間は短縮しますが、永らく待たされて結果がなかなか出ないことがあります。

#### 自分のPCとColabを繋ぐ
いつまでもずっと動かしたい場合、自分のPCにDockerで仮想環境を構築し、GoogleColabから接続する方法です。この場合、ノートを自分でローカル環境で動かしても良い、ということにはなるので、あまり意味は無いかも知れません。
やる方法は適宜、調べてください。

#### オススメしないがお金で解決
お金を払うとリソースを得られる方法が増えます。

#### 重課金
Pro+にするとブラウザを閉じても実行を続けます。GPUの性能や保証も変わります。

#### 他のCloud
仮想環境を他のものにすれば更に高性能化は可能です。

#### お金で解決することは技術がいらない
一応、提示はしましたが、高性能な環境で動作するのは当たり前で、工夫も要りません。低性能の環境で動作することで、中身を精査することが求められ、それがモデルの性能を上げることもあります。

---

## ４ 😩とにかくわけがわからない😩
色々やってて混乱し、何がなんだかわからないまま暗中模索から脱しない、そういう場合は以下を参考にしてください。

### LLMはあてになる
知見はかなりあります、バグの在るコードを吐くことはありかすが、使い方次第です。特に、サンプルでわからないところで活用しましょう。


### 追加で勉強すると解りやすくなるもの
教師あり学習のバックボーンになる理論を抜本的に学習するなら、以下が有効です。

#### Pythonの基礎
他のプログラミング言語が分かっているならば、あまり要らないかも知れません。

#### 確率・統計
どのラベルをどうしたら良いかなどで標準化などをする際に知識としてあると便利です。理系の大学を出てる方は履修歴が在る方も居ると思います。なお、分散や標準偏差などは高校の数学でやってることになってます。評価関数のRMSEなども理解しやすくなります。

#### 線形代数
データフレームを行列に見立てて捉えた場合に有効です、ベクトルと空間の概念はわかってても損はないですが、知ってるとどんどん高性能のモデルができる、というほどではありません。大学を出てる方なら、当時の教科書等が役立つ部分があると思います。

#### Kaggleの解説本
統合的に紹介から改善方法の解説が中心です、コードサンプルや実際の事例があると役に立ちます。実際に2冊、購入して読んでましたが、ここまで書いた内容と似通っていました。細かいパラメータチューンなどのコードの部分などは違いがあるものの、本を見ないで色々調べながらやっても王道の方法論は同じようです。

---

## ５ まとめ：基本は地道にやるしかない
機械学習を機械にさせるから簡単というものでもなく、最適化し高スコアを目指すのは地道なものです。入賞者の記事を読んでみると、意外と複雑なことは最終的にはやっていない状態ですが、そこに至るまでのパラメータなどの細部の調整が多い印象です。
一朝一夕にはいかない前提で、少しづつ進めることをお勧めします。
